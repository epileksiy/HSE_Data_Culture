{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание к семинару 5-6\n",
    "\n",
    "\n",
    "Задача на kaggle: https://www.kaggle.com/c/bike-sharing-demand\n",
    "\n",
    "По историческим данным о прокате велосипедов и погодным условиям необходимо оценить спрос на прокат велосипедов.\n",
    "\n",
    "## Вводная: \n",
    "\n",
    "В этом задании мы на примерах увидим, как переобучаются линейные модели, разберем, почему так происходит, и выясним, как диагностировать и контролировать переобучение.\n",
    "\n",
    "Во всех ячейках, где написан комментарий с инструкциями, нужно написать код, выполняющий эти инструкции. Остальные ячейки с кодом (без комментариев) нужно просто выполнить. В самом конце вас ждёт блок из вопросов, связанный с интерпретпцией моделей. На него нужно ответить. \n",
    "\n",
    "Напоминаем, что посмотреть справку любого метода или функции (узнать, какие у нее аргументы и что она делает) можно с помощью комбинации Shift+Tab. Нажатие Tab после имени объекта и точки позволяет посмотреть, какие методы и переменные есть у этого объекта."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Как обычно, для начала подгружаем нужные нам пакеты\n",
    "import pandas as pd    # пакет для работы с таблицами \n",
    "import numpy as np     # пакет для работы с матрицами \n",
    "\n",
    "# пакеты для картиночек \n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('ggplot')  # правильный (наиболее красивый) стиль у графиков\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы будем работать с датасетом __bikes_rent.csv__, в котором по дням записаны календарная информация и погодные условия, характеризующие автоматизированные пункты проката велосипедов, а также число прокатов в этот день. Последнее мы будем предсказывать; таким образом, мы будем решать задачу регрессии."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Знакомство с данными\n",
    "\n",
    "Загрузите датасет с помощью функции __pandas.read_csv__ в переменную __df__. Выведите первые 5 строчек, чтобы убедиться в корректном считывании данных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ваш код\n",
    "\n",
    "### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для каждого дня проката известны следующие признаки (как они были указаны в источнике данных):\n",
    "\n",
    "* `season`: 1 - весна, 2 - лето, 3 - осень, 4 - зима\n",
    "* `yr`: 0 - 2011, 1 - 2012\n",
    "* `mnth`: от 1 до 12\n",
    "* `holiday`: 0 - нет праздника, 1 - есть праздник\n",
    "* `weekday`: от 0 до 6\n",
    "* `workingday`: 0 - нерабочий день, 1 - рабочий день\n",
    "* `weathersit`: оценка благоприятности погоды от 1 (чистый, ясный день) до 4 (ливень, туман)\n",
    "* `temp`: температура в Цельсиях\n",
    "* `atemp`: температура по ощущениям в Цельсиях\n",
    "* `hum`: влажность\n",
    "* `windspeed(mph)`: скорость ветра в милях в час\n",
    "* `windspeed(ms)`: скорость ветра в метрах в секунду\n",
    "* `cnt`: количество арендованных велосипедов (это целевой признак, его мы будем предсказывать)\n",
    "\n",
    "Итак, у нас есть вещественные, бинарные и номинальные (порядковые) признаки, и со всеми из них можно работать как с вещественными. С номинальныеми признаками тоже можно работать как с вещественными, потому что на них задан порядок. Давайте посмотрим на графиках, как целевой признак зависит от остальных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=3, ncols=4, figsize=(15, 10))\n",
    "for idx, feature in enumerate(df.columns[:-1]):\n",
    "    df.plot(feature, \"cnt\", subplots=True, kind=\"scatter\", ax=axes[idx // 4, idx % 4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Каков характер зависимости числа прокатов от месяца? От каких признаков спрос зависит линейно? \n",
    "\n",
    "Напоследок посмотрим средние признаков (метод mean), чтобы оценить масштаб признаков и доли 1 у бинарных признаков."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Признаки имеют разный масштаб, значит для дальнейшей работы нам лучше нормировать матрицу объекты-признаки.\n",
    "\n",
    "## 2. Проблема первая - корреляции.\n",
    "\n",
    "Постройте матрицу корреляций. Есть ли между какими-то признаки слишком высокая корреляция? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итак, в наших данных некоторые признаки дублируют друг-друга. Также много похожих признаков. Конечно, мы могли бы сразу удалить дубликаты, но давайте посмотрим, как бы происходило обучение модели, если бы мы не заметили эту проблему. \n",
    "\n",
    "Для начала проведем масштабирование, или стандартизацию признаков: из каждого признака вычтем его среднее и поделим на стандартное отклонение. Это можно сделать с помощью метода scale. Кроме того, нужно перемешать выборку, это потребуется для кросс-валидации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import scale\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "df_shuffled = shuffle(df, random_state=123)       # перемешали \n",
    "X = scale(df_shuffled[df_shuffled.columns[:-1]])  # скалируем сразу всё (это не очень правильно)\n",
    "y = df_shuffled[\"cnt\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте обучим линейную регрессию на наших данных и посмотрим на веса признаков."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы видим, что веса при линейно-зависимых признаках по модулю значительно больше, чем при других признаках. \n",
    "\n",
    "Чтобы понять, почему так произошло, можно залезть в математику и понять, что при скорелированных признаках, существует бесконечное число оптимальных коэффициентов. Такую ситуацию называют _проблемой мультиколлинеарности_.\n",
    "\n",
    "С парой `temp`-`atemp` чуть менее коррелирующих переменных такого не произошло, однако на практике всегда стоит внимательно следить за коэффициентами при похожих признаках.\n",
    "\n",
    "__Решение__ проблемы мультиколлинеарности состоит в _регуляризации_ линейной модели. К оптимизируемому функционалу прибавляют L1 или L2 норму весов, умноженную на коэффициент регуляризации $\\alpha$. В первом случае метод называется Lasso, а во втором - Ridge. \n",
    "\n",
    "Обучите регрессоры Ridge и Lasso с параметрами по умолчанию и убедитесь, что проблема с весами решилась."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso, Ridge  # подгружаем модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###  Lasso модель\n",
    "### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###   Ridge модель\n",
    "### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вспомним семинар. Поясните каким образом введение регуляризации решает проблему с весами и мультиколлинеарностью."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Проблема вторая: неинформативные признаки\n",
    "\n",
    "В отличие от L2-регуляризации, L1 обнуляет веса при некоторых признаках. Давайте пронаблюдаем, как меняются веса при увеличении коэффициента регуляризации $\\alpha$ (в лекции коэффициент при регуляризаторе мог быть обозначен другой буквой)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = np.arange(1, 500, 50)   # задали диапазон для силы регуляризатора\n",
    "\n",
    "# матрицы для весов перед коэффициентами (число регрессоров)*(число признаков)\n",
    "coefs_lasso = np.zeros((alphas.shape[0], X.shape[1]))\n",
    "coefs_ridge = np.zeros((alphas.shape[0], X.shape[1]))\n",
    "\n",
    "i = 0\n",
    "for alph in alphas:\n",
    "    rg = Ridge(alpha=alph)  # для каждого alph обучаем модель\n",
    "    ls = Lasso(alpha=alph)\n",
    "    rg.fit(X,y)\n",
    "    ls.fit(X,y)\n",
    "    coefs_ridge[i] = rg.coef_  # и запоминаем коэффициенты\n",
    "    coefs_lasso[i] = ls.coef_\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Визуализируем динамику весов при увеличении параметра регуляризации:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "for coef, feature in zip(coefs_lasso.T, df.columns):\n",
    "    plt.plot(alphas, coef, label=feature, color=np.random.rand(3))\n",
    "plt.legend(loc=\"upper right\", bbox_to_anchor=(1.4, 0.95))\n",
    "plt.xlabel(\"alpha\")\n",
    "plt.ylabel(\"feature weight\")\n",
    "plt.title(\"Lasso\")\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "for coef, feature in zip(coefs_ridge.T, df.columns):\n",
    "    plt.plot(alphas, coef, label=feature, color=np.random.rand(3))\n",
    "plt.legend(loc=\"upper right\", bbox_to_anchor=(1.4, 0.95))\n",
    "plt.xlabel(\"alpha\")\n",
    "plt.ylabel(\"feature weight\")\n",
    "plt.title(\"Ridge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Картинки вышли красивыми. Обратите внимание, что регуляризатор Lasso агрессивнее уменьшает веса. Подумайте с чем это связано. Если делать alpha очень большим, веса зануляются. В случае Ridge такого не происходит. Веса становятся всё ближе к нулю, но зануления не происходит. \n",
    "\n",
    "Из-за того, что Lasso зануляет некоторые признаки, его можно использовать для отбора самых важных признаков. Дальше будем работать именно с ним. \n",
    "\n",
    "Итак, мы видим, что при изменении alpha модель по-разному подбирает коэффициенты признаков. Нам нужно выбрать наилучшее alpha. \n",
    "\n",
    "Для этого, во-первых, нам нужна метрика качества. Будем использовать в качестве метрики сам оптимизируемый функционал метода наименьших квадратов, то есть MSE (Mean Square Error)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Во-вторых, нужно понять, на каких данных эту метрику считать. Нельзя выбирать `alpha` по значению MSE на обучающей выборке, потому что тогда мы не сможем оценить, как модель будет делать предсказания на новых для нее данных. Если мы выберем одно разбиение выборки на обучающую и тестовую (это называется holdout), то настроимся на конкретные \"новые\" данные, и вновь можем переобучиться. Поэтому будем делать несколько разбиений выборки, на каждом пробовать разные значения `alpha`, а затем усреднять MSE. Удобнее всего делать такие разбиения кросс-валидацией, то есть разделить выборку на $K$ частей, или блоков, и каждый раз брать одну из них как тестовую, а из оставшихся блоков составлять обучающую выборку. \n",
    "\n",
    "Подбирать параметр alpha в `sklearn` совсем просто: для этого есть , `GridSearchCV`. Мы уже сталкивались с примером применения этой функции на семинаре. Попробуйте с помощью неё перебрать список из альф и подыскать оптимальное. Параметр cv отвечает за то на сколько часте делится выборка. Поставьте `cv=5`. Также укажите параметр `scoring = 'mean_squared_error'`. Это задаст функцию потерь, на которую будет ориентироваться `GridSearchCV` при переборе. Назовите переменную, в которой будет находиться перебор grid_cv_lasso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "\n",
    "# Обучите регрессор LassoCV на всех параметрах регуляризации из alpha\n",
    "# Постройте график _усредненного_ по строкам MSE в зависимости от alpha. \n",
    "# Выведите выбранное alpha, а также пары \"признак-коэффициент\" для обученного вектора коэффициентов\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# если вы всё сделали верно, тут появится картинка с оптимальным альфа и динамикой ошибки. \n",
    "mean_mses = [list(item)[1] for item in grid_cv_lasso.grid_scores_]\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(alphas, mean_mses)\n",
    "plt.xlabel(\"alpha\")\n",
    "plt.ylabel(\"Mean MSE\")\n",
    "plt.title(\"Optimal alpha\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведите значения получившихся коэффициентов. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наконец, как принято в анализе данных, давайте проинтерпретируем результат.\n",
    "\n",
    "Проинтерпретируйте последнюю обученую модель. У каких признаков наибольшие положительные коэфициенты? У каких наибольшие отрицательные? Логично ли утверждать, что чем больще/меньше эти признаки, тем выше/ниже спрос на велосипеды? Какие коэффициенты занулились? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Заключение\n",
    "Итак, мы посмотрели, как можно следить за адекватностью линейной модели, как отбирать признаки и как грамотно, по возможности не настраиваясь на какую-то конкретную порцию данных, подбирать коэффициент регуляризации. \n",
    "\n",
    "Стоит отметить, что с помощью кросс-валидации удобно подбирать лишь небольшое число параметров (1, 2, максимум 3), потому что для каждой допустимой их комбинации нам приходится несколько раз обучать модель, а это времязатратный процесс, особенно если нужно обучаться на больших объемах данных."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
